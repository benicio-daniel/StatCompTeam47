---
title: "Case Study 1"
subtitle: "AKSTA Statistical Computing"
output: 
#pdf_document
  unilur::examen_pdf_solution: default
  #unilur::examen_pdf: default
  keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*The  .Rmd and .html (alternatively the .pdf) should be uploaded in TUWEL by the deadline. Refrain from using explanatory comments in the R code chunks but write them as text instead. Points will be deducted if the submitted file is not in a decent form.*


# 2. Gamma function

## a. 
Write a function to compute the following for $n$ a positive integer
using the `gamma` function in base R.
$$
\rho_n =\frac{\Gamma((n-1)/2)}{\Gamma(1/2)\Gamma((n - 2)/2)}
$$

```{r, eval = TRUE}
rho_n <- function(n) {
    if (n < 2) {
        print("n must be > 2")
        return(NA)
    } else if (!is.numeric(n)) {
        print("n must be numeric")
        return(NA)
    }

  numerator <- gamma((n - 1) / 2)
  denominator <- gamma(1 / 2) * gamma((n - 2) / 2) # this is why n must be > 2
  result <- numerator / denominator

  return(result)
}
```

## b. 

Try $n = 2000$. What do you observe? Why do you think the observed behavior happens?

```{r, eval = TRUE}
rho_n(2000)
gamma(2000)
```
As observed, the gamma function returns 'Inf' for n = 2000, resulting in an undefined Inf/Inf expression, which is evaluated as NaN.

## c.
Write an implementation which can also deal with large values of $n>1000$.

```{r, eval = TRUE}
rho_n_log <- function(n) {
    if (n < 2) {
        print("n must be > 2")
        return(NA)
    } else if (!is.numeric(n)) {
        print("n must be numeric")
        return(NA)
    }

  numerator <- lgamma((n - 1) / 2) 
  denominator <- lgamma(1 / 2) + lgamma((n - 2) / 2)
  result <- exp(numerator - denominator)
  return(result)
}
```
The function `lgamma(n)` computes the natural logarithm of the gamma function, i.e., $\log(\Gamma(n))$. 

This is equivalent to computing `log(gamma(n))`, but it has a key advantage: it uses the **Stirling approximation** in its logarithmic form **before** performing the computation, rather than applying the logarithm afterwards. This avoids the numerical overflow that can occur when computing `gamma(n)` directly for large values of $n$.

Specifically, the approximation used is:

$$
\log(\Gamma(n)) \approx \left(n - \frac{1}{2}\right)\log(n) - n + \frac{1}{2}\log(2\pi)
$$

This form grows much more slowly as $n \to \infty$ compared to first computing:

$$
\Gamma(n) \approx \sqrt{2\pi} \, n^{n - \frac{1}{2}} e^{-n}
$$

and then taking the logarithm. By working directly with the logarithmic form, the computation remains numerically stable even for very large $n$.

```{r, eval = TRUE}
rho_n_log(2000)
```

## d. 
Plot $\rho_n/\sqrt{n}$ for different values of $n$. Can you guess the limit of $\rho_n/\sqrt{n}$ for $n \rightarrow\infty$?

```{r, eval = TRUE}
library(ggplot2)

# Compute rho_n / sqrt(n) for values of n
n_vals <- 3:2000  # rho_n_log is not defined for n < 2
rho_vals <- sapply(n_vals, function(n) rho_n_log(n) / sqrt(n))

# Create data frame for plotting
df <- data.frame(n = n_vals, rho_over_sqrt_n = rho_vals)

# Plot with ggplot2
plot <- ggplot(df, aes(x = n, y = rho_over_sqrt_n)) +
  geom_line(color = "red") +
  scale_x_log10() +
  labs(x = "n",
       y = expression(frac(rho[n], sqrt(n))))

# Show the plot
print(plot)
```
Using Stirling's approximation for large arguments of the gamma function and the fact that $\Gamma(1/2) = \sqrt{\pi}$, we find that the two $n$-dependent gamma functions in the expression for $\rho_n$ simplify to approximately $\sqrt{n}$.

This is because the ratio of two gamma functions with arguments differing by $1$ behaves asymptotically like the square root of their argument. More precisely:

$$
\frac{\Gamma\left(\frac{n - 1}{2}\right)}{\Gamma\left(\frac{n - 2}{2}\right)} \sim \sqrt{\frac{n - 2}{2}} \quad \text{as } n \to \infty
$$

Therefore, when computing $\frac{\rho_n}{\sqrt{n}}$, the square roots cancel out, and we are left with:

$$
\frac{\rho_n}{\sqrt{n}} \to \frac{1}{\sqrt{\pi}} \quad \text{as } n \to \infty
$$


# 4. Central limit theorem

The central limit theorem states that, under certain conditions, the sum of a large number of random variables is approximately normal. Consider the case where
$X_1, X_2,\ldots,X_n$ are i.i.d. random variables
with finite expected values ($E(X_i)=\mu<\infty$)
 and variances $(Var(X_i)=\sigma^2<\infty)$.
The central limit theorem states that the cumulative distribution function  (CDF)
of the normalized random variable
$$
Z_n = \frac{\bar X_n - \mu}{\sigma/\sqrt{n}}
$$
converges to the standard normal CDF as $n\rightarrow\infty$.

## a. 
Derive the expected value of the sample mean $\bar X_n=\frac{X_1+X_2+\ldots+X_n}{n}$.

To derive the expected value of the sample mean $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$, we use the linearity of expectation:

$$
\mathbb{E}[\bar{X}_n] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} X_i\right] = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[X_i]
$$

Since all $X_i$ are independent and identically distributed with $\mathbb{E}[X_i] = \mu$, this simplifies to:

$$
\mathbb{E}[\bar{X}_n] = \frac{1}{n} \cdot n \cdot \mu = \mu
$$

## b. 
Derive the variance of the sample mean $\bar X_n=\frac{X_1+X_2+\ldots+X_n}{n}$.

To derive the variance of the sample mean $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$, we use the rule for scaling variances:

$$
\text{Var}(aX) = a^2 \cdot \text{Var}(X)
$$

Applying this to the sample mean:

$$
\text{Var}(\bar{X}_n) = \text{Var}\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n^2} \cdot \text{Var}\left( \sum_{i=1}^{n} X_i \right)
$$

Since the $X_i$ are independent and identically distributed, the variance of their sum is the sum of their variances:

$$
\text{Var}\left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} \text{Var}(X_i) = n\sigma^2
$$

Putting it all together:

$$
\text{Var}(\bar{X}_n) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}
$$

## c. 
The central limit theorem makes few assumption on the distribution of the random variables $X_i$.
Consider again the die-rolling experiment where $X_i$
is the random variable giving the number in roll $i$.

- Simulate Die Rolls: For each value of $n$ (where 
$n=\{100,1000,10000\}$), simulate rolling a die 
$n$ times.
Record the mean of the $n$ rolls.

- Repeat the above process $M=100$ times for each value of
$n$. This will give you 100 sample means for each $n$.

- Plot a histogram of the 100 sample means for each value of $n$. Observe the shape of the histograms.

- Discuss how the distribution of the sample means changes as $n$ increases. Comment on whether the histograms appear to approach a normal distribution as $n$, illustrating the Central Limit Theorem.

```{r, eval = TRUE}
# Set seed for reproducibility
set.seed(123)
```
```{r, eval = TRUE, fig.width=10, fig.height=3}
# Parameters
n_values <- c(100, 1000, 10000) # sample sizes
M <- 100  # number of repetitions

# For storing results
sample_means_list <- list()

# Simulate die rolls and store sample means
for (n in n_values) {
  sample_means <- replicate(M, mean(sample(1:6, size = n, replace = TRUE)))
  sample_means_list[[as.character(n)]] <- sample_means # List with keynames
}

# Plot histograms
par(mfrow = c(1, 3))  # 1 row, 3 plots by the keys

for (n in n_values) {
    means <- sample_means_list[[as.character(n)]]
  hist(sample_means_list[[as.character(n)]],
       main = paste("n =", n),
       xlab = "Sample Mean",
       probability = TRUE  # y-axis will represent probabilities, not counts
       )
  
  # Overlay normal curve
  # Compute the mean and standard deviation of the sample means
  mean_val <- mean(means)
  sd_val <- sd(means)
  
  # Generate x values for the normal distribution curve
  x_vals <- seq(min(means), max(means), length = 100)
  y_vals <- dnorm(x_vals, mean = mean_val, sd = sd_val)
  
  # Draw the normal curve on top of the histogram
  lines(x_vals, y_vals, col = "red", lwd = 2)
}
```
As the sample size $n$ increases, the distribution of the sample means becomes more concentrated around the true mean and shows less spread. The histograms increasingly resemble a normal distribution, illustrating the Central Limit Theorem in action.

## d. 
Compute expected value $\mu$ and the variance  $\sigma^2$ for the random variable $X_i$ (giving the number in die-roll $i$), which follows a discrete uniform distribution.

```{r, eval = TRUE}
# Values for a fair die
die_values <- 1:6

# Expected value (mean)
mu <- mean(die_values)

# Variance
sigma_squared <- sum((die_values - mu)^2) / length(die_values)

# Output
cat("Expected value (μ):", mu, "\n")
cat("Variance (σ²):", sigma_squared, "\n")
```

## e.
Now that you computed $\mu$ and $\sigma^2$, 
consider $Z_n$.

- For each value of  $n$ (where $n=\{100,1000,10000\}$)
 and for 
$M=\{100,1000,10000\}$ replications, simulate the rolling of $n$ dice and calculate the $Z_n$.

- Plot histograms of $Z_n$ for each combination of 
$n$ and  $M$.

- Discuss how the distribution of $Z_n$
changes as  $n$  and  $M$ increase.
Comment on whether the distributions of 
$Z_n$ appear to approach a normal distribution, illustrating the Central Limit Theorem.

```{r, eval = TRUE}
# Parameters
n_values <- c(100, 1000, 10000)
M_values <- c(100, 1000, 10000)
sigma = sqrt(sigma_squared)

# For storing results
zn_results <- list()

# Loop over n and M
for (n in n_values) {
  for (M in M_values) {
    
    # Simulate M means of n dice rolls
    sample_means <- replicate(M, mean(sample(1:6, size = n, replace = TRUE)))
    
    # Standardize to get Z_n
    zn <- (sample_means - mu) / (sigma / sqrt(n))
    
    # Store in list
    key <- paste0("n=", n, "_M=", M)
    zn_results[[key]] <- zn
  }
}
````

```{r, eval = TRUE, fig.width=10, fig.height=10}
# Plot histograms
par(mfrow = c(3, 3))  # 3 row, 3 plots by the keys

for (key in names(zn_results)) {
  hist(zn_results[[key]],
       probability = TRUE,
       main = key,
       xlab = "Z_n")
  
  # Add standard normal curve
  x_vals <- seq(-4, 4, length = 100)
  y_vals <- dnorm(x_vals, mean = 0, sd = 1)
  lines(x_vals, y_vals, col = "red", lwd = 2)
}
```

As the sample size n increases, the standardized sample means Zₙ become more concentrated around 0 and resemble a normal distribution more closely. A larger number of replications M results in smoother histograms. These observations confirm the classical Central Limit Theorem, which states that the distribution of independent and identically distributed random variables with finite mean and variance approaches the standard normal distribution as n approaches infinity.